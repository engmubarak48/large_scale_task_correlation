datamodule:
  args_data: 
      coco:
        val_images_path: "./coco_dataset/images/val2017"
        val_annotation_path: "./coco_dataset/annotations/instances_val2017.json"
        train_images_path: "./coco_dataset/images/train2017"
        train_annotation_path: "./coco_dataset/annotations/instances_train2017.json"
        # mean and std stats
        stats: {"mean": [0.485, 0.456, 0.406], "std": [0.229, 0.224, 0.225]}
        resize_size: 256
        num_classes: 80
        strategy: "random"
      celebA:
        val_images_path: "../large_scale_task_correlation/data/celeb_datasets"
        train_images_path: "../large_scale_task_correlation/data/celeb_datasets"
        # mean and std stats
        crop_size: 56
        stats: {"mean": [0.485, 0.456, 0.406], "std": [0.229, 0.224, 0.225]}
        resize_size: 64
        strategy: "random"
        pretrain_blocks: []
        finetune_blocks: []
        json_labels_file: "random_strategy_labels.json"
    
  args_train_test:
    dataset_name: "celebA"
    mode: "pretrain"
    scheduler_name: CosineAnnealingLR
    batch_size: 256
    train_batch_jump: 100
    test_batch_jump: 100
    epochs: 200
    lr: 0.001
    momentum: 0.9
    weight_decay: 0.0001
    num_workers: 128
    patience: 7
    finetune_params:
      lr: 0.0001
      momentum: 0.9
      weight_decay: 0.0004
    CosineAnnealingLR:
      T_max: 10
      eta_min: 0
    ExponentialLR: 
      gamma: 0.1
    ReduceLROnPlateau: 
      mode: "min"
      factor: 0.1
      patience: 10
  args_result: 
    project_name: "lsc"
    pretrained_models_path: "../large_scale_task_correlation/src/outputs/checkpoint/pretrain" 
    finetuned_models_path: "../large_scale_task_correlation/src/outputs/checkpoint/finetune" 
    logPath: "../large_scale_task_correlation/src/outputs/logs"
    load_ckpts: ""
    max_files_to_keep: 2
    wandb_log: True
    save_ckpts: True


architecture:     # The parameters for the architecture
